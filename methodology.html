<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>CT Scan</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar bg-body-tertiary navbar-expand-xxl fixed-top">
        <div class="container-fluid">
          <a class="navbar-brand" href="#">
            <img src="icons/medical-team.png" alt="Logo" width="30" height="24" class="d-inline-block align-text-top">
            CT Scan
          </a>

          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link" aria-current="page" href="index.html">Introduction</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="dataset.html">Dataset and Problem</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link active" href="methodology.html" text="#ffffff">Methodology and Experiment</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="result.html">Conclusion and Future Work</a>
                </li>
            </ul>
          </div>
        </div>
    </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/meth_back.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="page-heading">
                            <h1 class = "topic_text"> Methodology and Experiment</h1>
                            <span class="subheading"></span>
                        </div>
                    </div>
            </div>
        </header>
        <!-- Main Content-->
        <main class="mb-4">
            <div class="container">
                <div class="text-center mt-5">
                </div>
                <img src="assets/img/overview.jpg" alt="Flowers in Chania">
            </div>
            <section class="gradient-custom-5">
                <div class="container py-5">
                <div class="main-timeline-5">
                  <p class="lead">  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Click for more details below </p>
                  <br>
                  <div class="timeline-5 right-5">
                    <div class="card">
                      <div class="card-body p-4">
                        <h5> <a href="#method1" class="lead"> Tokenization/Parsing</a></h5>
                        <p class="lead"> Remove stopwords, lemmatization, stemming</p>
                      </div>
                    </div>
                  </div>
                  <div class="timeline-5 right-5">
                    <div class="card">
                      <div class="card-body p-4">
                        <h5><a href="#method2" class="lead"> Search System and techniques</a></h5>
                        <p class="lead"> BM25, Demographic Filtering. Relevance Boosting with Medical NER</p>
                      </div>
                    </div>
                  </div>
                  <div class="timeline-5 right-5">
                    <div class="card">
                      <div class="card-body p-4">
                        <h5><a href="#method3" class="lead"> Re-ranking System</a></h5>
                        <p class="lead"> MonoBERT, DuoBERT</p>
                      </div>
                    </div>
                  </div>
                  <div class="timeline-5 right-5">
                    <div class="card">
                      <div class="card-body p-4">
                        <h1><a href="#method3" class="lead"> Adhoc Query Generation with T5 model</a></h1>
                        <p class="lead">SIGIR query pairs for training, result boosting</p>
                      </div>
                    </div>
                  </div>
                </div>
                </div>
                </section>

        <div class="container">
                <div id = "method1" class="text-center mt-5">
                      <h2>Tokenization/Parsing</h2>
                </div>
                <p class="lead"> Depending on the specific structure of the documents,
                  we perform data preprocessing steps including removing stopwords, lemmatization, and stemming. </p>
        </main>
                <br>
                                    <hr>
        <div class="container">
                <div id = "method2" class="text-center mt-5">
                      <h2>Search System</h2>
                </div>
                <ul class="lead">
                        <li>BM25</li>
                        <p class="lead2">Introduced by <a href = "https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf">
                          Robertson and Jones </a>, BM25 is a widely used ranking function calculating the relevance of a document to a query.
                          Meanwhile, BM25 also considers the frequency and distribution of query terms in the document.</p>
                        <li>Demographic Filtering (DF)</li>
                        <p class="lead2"> DF uses users' demographic data for ranking
                          in the recommendation system. In our project, we apply it by extracting the minimum age, maximum age and gender from documents while
                          extracting age, gender of the patient with Named Entity Recognition (NER).
                           In addition, trials that does not match patient demographics are moved to the bottom of the initial ranking list/penalized by a factor.</p>
                        <li>Relevance Boosting with Medical NER</li>
                        <p class="lead2"> Keywords and condition section of clinical trials are extracted.
                          We use biomedical NER to retrieve Biological_structure, Disease_disorder, Sign_symptom, History entities from doc.
                          In addition, trials that match patient medical entities are boosted based on the number of matches. </p>
                </ul>
        </main>
                <br>
                                    <hr>
        <div class="container">
                <div id = "method3" class="text-center mt-5">
                      <h2>Re-ranking System</h2>
                </div>
                <ul class="lead">
                  <p class="lead"> We applied two <a href = "https://aclanthology.org/N19-1423.pdf">
                    BERT</a>-based models -- MonoBERT and DuoBERT -- introduced by
                    <a href = "https://arxiv.org/pdf/1910.14424.pdf"> Nogueria et al. in 2019 </a>.
                     MonoBERT and DuoBERT formulate the ranking problem as pointwise
                     and pairwise classification respectively with more details below.

                        <li>MonoBERT</li>
                          <p class="lead2"> MonoBERT is a BERT encoder with a single
                            neuron output layer connected to the encoder's pooled layer with dropout.
                            In addition to the pointwise approach, the re-rankers are
                            also trained using a cross-entropy loss.</p>
                          <p class="lead2"> Target labels are derived from corresponding relevance
                            judgements of the training datasets. For SIGIR - revelance lables 0 constitutes
                          negative exmaples while 1 and 2 constitute positive examples.
                          For TREC - relevance labels 0 and 1 constitute 0 and 1 constitute negative examples
                          and label 2 constitutes positive examples. </p>
                          <p class="lead2"> We re-rank the top 100 documents according to the predicted raw score. </p>
                        <li>DuoBERT</li>
                          <p class="lead2"> DuoBERT adopted a pairwise approach that compares
                          pairs of documents. To illustrate, the re-reanker estimates the probability one candidate is more relevant to the other, denoted by
                          <b>P(d<sub>i</sub> >d<sub>j</sub> | q, d<sub>i</sub>, d<sub>j</sub>) </b> where <b>d<sub>i</sub> >d<sub>j</sub></b> meaning
                          <b>d<sub>i</sub></b> is more relevant than <b>d<sub>j</sub></b>. </p>
                          <p class="lead2">We use a pretrained sentence transformer trained on <a href="https://nlp.stanford.edu/pubs/snli_paper.pdf">SNLI</a>,
                          <a href="https://arxiv.org/pdf/1704.05426.pdf">MNLI</a>,
                          <a href = "https://jgc128.github.io/mednli/">MEDNLI</a> and
                          <a href = "https://aclanthology.org/2022.acl-long.511/"> SCINLI</a> to generate embeddings.</p>
                          <p class="lead2"> Cosine similarity between query and 2 documents are calculated and negated.</p>
                </ul>
        </main>
        <br>
                                    <hr>
        <div class="container">
                <div id = "method4" class="text-center mt-5">
                      <h2>Adhoc Query Generation with T5 model</h2>
                </div>
                <p class="lead"> A <a href = "https://arxiv.org/pdf/1910.10683.pdf"> Text-to-Text Transfer Transformer (T5)</a>-base
                   model is finetuned for query generation. We trained the model on SIGIR (description, ad-hoc) query pairs.</p>
                <p class="lead"> We found that the results often contain excerpts from the description. In addition, we boosted the results
                  for TREC but Synthetic queries performs worse for SIGIR. </p>
        </main>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <script src="js/scripts.js"></script>
    </body>
</html>
